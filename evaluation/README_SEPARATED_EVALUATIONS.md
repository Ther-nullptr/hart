# HART Evaluation Scripts - Separated Evaluations

This directory now contains **separated evaluation scripts** that allow you to run FID and GenEval evaluations independently, providing more flexibility and focused evaluation workflows.

## üìÅ Available Scripts

### üéØ Individual Evaluation Scripts

1. **`run_fid_evaluation.sh`** - FID evaluation only
2. **`run_geneval_evaluation.sh`** - GenEval evaluation only

### üîÑ Legacy Scripts (Still Available)

3. **`run_hart_evaluation_corrected.sh`** - Combined evaluation (both FID + GenEval)

## üöÄ Quick Start Guide

### FID Evaluation Only

```bash
# Full MJHQ-30K FID evaluation
./run_fid_evaluation.sh --model-path /path/to/hart/model \
                       --mjhq-metadata /path/to/MJHQ30K/meta_data.json \
                       --mjhq-images /path/to/MJHQ30K/mjhq30k_imgs

# Direct FID computation between two directories
./run_fid_evaluation.sh --direct-fid /path/to/real /path/to/generated

# Quick test with limited samples
./run_fid_evaluation.sh --max-samples 100 --category-filter people
```

### GenEval Evaluation Only

```bash
# Full GenEval evaluation (downloads official dataset)
./run_geneval_evaluation.sh --model-path /path/to/hart/model

# Quick test with limited prompts
./run_geneval_evaluation.sh --max-prompts 100

# Evaluate existing images only
./run_geneval_evaluation.sh --evaluate-only /path/to/prompts \
                           --detector-checkpoint /path/to/checkpoint.pth
```

## üìä Detailed Usage

### FID Evaluation Script (`run_fid_evaluation.sh`)

#### Purpose
Evaluates image quality using Fr√©chet Inception Distance on MJHQ-30K dataset.

#### Two Modes

**1. Full MJHQ-30K Evaluation**
```bash
./run_fid_evaluation.sh \
    --model-path /path/to/hart/model \
    --mjhq-metadata /path/to/MJHQ30K/meta_data.json \
    --mjhq-images /path/to/MJHQ30K/mjhq30k_imgs \
    --category-filter people \
    --max-samples 1000
```

**2. Direct FID Mode**
```bash
./run_fid_evaluation.sh --direct-fid /path/to/real /path/to/generated
```

#### Key Options
- `--model-path`: Path to HART model (required for generation mode)
- `--mjhq-metadata`: Path to MJHQ metadata JSON
- `--mjhq-images`: Path to MJHQ images directory
- `--direct-fid`: Compute FID between two existing directories
- `--category-filter`: Filter to specific category (people, animals, etc.)
- `--max-samples`: Limit number of samples
- `--cfg`: CFG scale for generation
- `--seed`: Random seed

#### Output
```
üìä FID Evaluation Results:
üéØ FID Score: 15.2340
üìã Evaluation Details:
  ‚Ä¢ Samples Evaluated: 1000
  ‚Ä¢ Category Filter: people
  ‚Ä¢ Generation Config: CFG=4.5, Size=1024px
```

### GenEval Evaluation Script (`run_geneval_evaluation.sh`)

#### Purpose
Evaluates compositional reasoning using the official GenEval dataset.

#### Three Modes

**1. Full Evaluation (Generate + Evaluate)**
```bash
./run_geneval_evaluation.sh \
    --model-path /path/to/hart/model \
    --max-prompts 500
```

**2. Evaluate Existing Images Only**
```bash
./run_geneval_evaluation.sh \
    --evaluate-only /path/to/prompts \
    --detector-checkpoint /path/to/checkpoint.pth
```

**3. Generate Images Only**
```bash
./run_geneval_evaluation.sh \
    --skip-generation \
    --model-path /path/to/hart/model
```

#### Key Options
- `--model-path`: Path to HART model (required for generation)
- `--detector-checkpoint`: Path to detection model checkpoint
- `--max-prompts`: Limit prompts for testing
- `--evaluate-only`: Only evaluate existing images
- `--skip-generation`: Skip image generation step
- `--images-per-prompt`: Number of images per prompt

#### Output
```
üéØ GenEval Evaluation Results:
üéØ Overall Accuracy: 0.756
üìã Task-specific Results:
  ‚Ä¢ Single Object  : 0.892 (1250/1401)
  ‚Ä¢ Two Object     : 0.734 (445/606)
  ‚Ä¢ Counting       : 0.623 (187/300)
  ‚Ä¢ Colors         : 0.801 (561/700)
  ‚Ä¢ Position       : 0.567 (340/600)
  ‚Ä¢ Color Attr     : 0.689 (206/299)
```

## ‚öôÔ∏è Configuration

### Script Configuration Sections

Both scripts have configuration sections at the top:

```bash
# =============================================================================
# Configuration - Update these paths for your setup
# =============================================================================

MODEL_PATH="/path/to/hart/model"  # Update this
# ... other settings
```

### Environment Variables

You can also set paths via environment variables:
```bash
export HART_MODEL_PATH="/path/to/hart/model"
export MJHQ_METADATA="/path/to/MJHQ30K/meta_data.json"
export MJHQ_IMAGES="/path/to/MJHQ30K/mjhq30k_imgs"
```

## üîß Advanced Usage

### Batch Processing

**FID Evaluation on Multiple Categories**
```bash
for category in people animals vehicles; do
    ./run_fid_evaluation.sh \
        --category-filter $category \
        --output-dir ./results_$category
done
```

**GenEval with Different Settings**
```bash
for cfg in 3.0 4.5 6.0; do
    ./run_geneval_evaluation.sh \
        --cfg $cfg \
        --output-dir ./results_cfg_$cfg
done
```

### Pipeline Integration

**Sequential Evaluation**
```bash
# Run FID first
./run_fid_evaluation.sh --model-path /path/to/model

# Then run GenEval
./run_geneval_evaluation.sh --model-path /path/to/model
```

**Parallel Evaluation** (if you have multiple GPUs)
```bash
# Terminal 1
CUDA_VISIBLE_DEVICES=0 ./run_fid_evaluation.sh --device cuda:0 &

# Terminal 2  
CUDA_VISIBLE_DEVICES=1 ./run_geneval_evaluation.sh --device cuda:1 &
```

## üìà Results Interpretation

### FID Scores
- **< 10**: Excellent quality, very similar to real images
- **10-20**: Good quality, minor differences
- **20-50**: Moderate quality, noticeable differences  
- **> 50**: Poor quality, significant differences

### GenEval Scores
- **> 0.8**: Excellent compositional understanding
- **0.6-0.8**: Good performance with room for improvement
- **0.4-0.6**: Moderate performance, significant issues
- **< 0.4**: Poor compositional understanding

### Task-Specific Analysis

**GenEval Tasks**:
- **Single Object**: Basic object recognition capability
- **Two Object**: Object co-occurrence understanding
- **Counting**: Numerical reasoning (1-5 objects)
- **Colors**: Color attribute binding
- **Position**: Spatial relationship understanding
- **Color Attr**: Multi-object color attribute binding

## üõ†Ô∏è Troubleshooting

### Common Issues

**1. Model Not Found**
```bash
‚ùå ERROR: HART model directory not found
```
Solution: Update `MODEL_PATH` in script or use `--model-path`

**2. MJHQ Dataset Missing**
```bash
‚ùå ERROR: MJHQ metadata file not found
```
Solution: Download MJHQ-30K from https://huggingface.co/datasets/playgroundai/MJHQ-30K

**3. Detection Checkpoint Missing**
```bash
‚ö†Ô∏è Detection checkpoint not found
```
Solution: Let script auto-download or manually download Mask2Former checkpoint

**4. CUDA Out of Memory**
```bash
RuntimeError: CUDA out of memory
```
Solution: Reduce batch sizes in script configuration

### Debug Mode

Add `--help` to any script for detailed usage:
```bash
./run_fid_evaluation.sh --help
./run_geneval_evaluation.sh --help
```

### Logging

Scripts provide detailed logging. For even more verbose output:
```bash
bash -x ./run_fid_evaluation.sh  # Debug mode
```

## üîÑ Migration from Combined Script

### Before (Combined)
```bash
./run_hart_evaluation_corrected.sh
```

### After (Separated)
```bash
# Run individually as needed
./run_fid_evaluation.sh --model-path /path/to/model
./run_geneval_evaluation.sh --model-path /path/to/model
```

## üìÅ Output Structure

### FID Evaluation Output
```
fid_evaluation_results/
‚îú‚îÄ‚îÄ generated/              # Generated images
‚îú‚îÄ‚îÄ fid_results.json        # FID score and metadata
‚îî‚îÄ‚îÄ ...
```

### GenEval Evaluation Output
```
geneval_evaluation_results/
‚îú‚îÄ‚îÄ evaluation_metadata.jsonl   # Downloaded GenEval metadata
‚îú‚îÄ‚îÄ prompts/                    # Generated images in GenEval format
‚îÇ   ‚îú‚îÄ‚îÄ 0/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ metadata.jsonl
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ samples/
‚îÇ   ‚îú‚îÄ‚îÄ 1/
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ geneval_results.jsonl       # Detailed per-image results
‚îî‚îÄ‚îÄ geneval_summary.json        # Summary statistics
```

## ‚úÖ Benefits of Separation

1. **üéØ Focused Evaluation**: Run only the evaluation you need
2. **‚ö° Faster Development**: Skip time-consuming evaluations during development
3. **üîß Easier Debugging**: Isolate issues to specific evaluation types
4. **üìä Flexible Workflows**: Mix and match evaluations as needed
5. **üíæ Resource Management**: Control GPU memory usage better
6. **üîÑ Pipeline Integration**: Easier to integrate into automated workflows

---

**Choose the right script for your needs:**
- **FID only**: Image quality assessment ‚Üí `run_fid_evaluation.sh`
- **GenEval only**: Compositional reasoning ‚Üí `run_geneval_evaluation.sh`  
- **Both**: Complete evaluation ‚Üí `run_hart_evaluation_corrected.sh`